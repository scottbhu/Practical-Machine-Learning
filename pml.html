<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Practical Machine Learning Course Project</title>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>

<!-- MathJax scripts -->
<script type="text/javascript" src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}

pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Practical Machine Learning Course Project</h1>

<p>Load up the packages that will be used for this project</p>

<pre><code class="r">library(caret)
library(randomForest)
library(gbm)
library(plyr)
library(MASS)
</code></pre>

<p>Read the data into data frames</p>

<pre><code class="r">original_training &lt;- read.csv(&#39;pml-training.csv&#39;)

original_testing &lt;- read.csv(&#39;pml-testing.csv&#39;)
</code></pre>

<h3>Feature (Predictor) Selection</h3>

<p>There were several features (predictors) that contained mostly missing data. These predictors were not used in the model building process.</p>

<p>Summary of the training below suggests that there are multiple predictor that are empty or have values of DIV/0.</p>

<ul>
<li>These predictors will have to be removed.</li>
<li>This will be performed in a later step.</li>
<li>The results are hidden as it generates too much data.</li>
</ul>

<pre><code class="r">summary(original_training)
</code></pre>

<p>Create new training/testing sets that select out the features that do not have multiple missing values.</p>

<ul>
<li>These training and testing sets will have complete data.</li>
</ul>

<pre><code class="r">training &lt;- subset(original_training, select=c(user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window, roll_belt, pitch_belt, yaw_belt, total_accel_belt, gyros_belt_x, gyros_belt_y, gyros_belt_z, accel_belt_x, accel_belt_y, accel_belt_z, magnet_belt_x, magnet_belt_y, magnet_belt_z, roll_arm, pitch_arm, yaw_arm, total_accel_arm, gyros_arm_x, gyros_arm_y, gyros_arm_z, accel_arm_x, accel_arm_y, accel_arm_z, magnet_arm_x, magnet_arm_y, magnet_arm_z, roll_dumbbell, pitch_dumbbell, yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x, gyros_dumbbell_y, gyros_dumbbell_z, accel_dumbbell_x, accel_dumbbell_y, accel_dumbbell_z, magnet_dumbbell_x, magnet_dumbbell_y, magnet_dumbbell_z, roll_forearm, pitch_forearm, yaw_forearm, total_accel_forearm, gyros_forearm_x, gyros_forearm_y, gyros_forearm_z, accel_forearm_x, accel_forearm_y, accel_forearm_z, magnet_forearm_x, magnet_forearm_y, magnet_forearm_z, classe))

testing &lt;- subset(original_testing, select=c(user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window, roll_belt, pitch_belt, yaw_belt, total_accel_belt, gyros_belt_x, gyros_belt_y, gyros_belt_z, accel_belt_x, accel_belt_y, accel_belt_z, magnet_belt_x, magnet_belt_y, magnet_belt_z, roll_arm, pitch_arm, yaw_arm, total_accel_arm, gyros_arm_x, gyros_arm_y, gyros_arm_z, accel_arm_x, accel_arm_y, accel_arm_z, magnet_arm_x, magnet_arm_y, magnet_arm_z, roll_dumbbell, pitch_dumbbell, yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x, gyros_dumbbell_y, gyros_dumbbell_z, accel_dumbbell_x, accel_dumbbell_y, accel_dumbbell_z, magnet_dumbbell_x, magnet_dumbbell_y, magnet_dumbbell_z, roll_forearm, pitch_forearm, yaw_forearm, total_accel_forearm, gyros_forearm_x, gyros_forearm_y, gyros_forearm_z, accel_forearm_x, accel_forearm_y, accel_forearm_z, magnet_forearm_x, magnet_forearm_y, magnet_forearm_z, problem_id))
</code></pre>

<p>Change user_name to factor</p>

<pre><code class="r">training$user_name &lt;- factor(training$user_name)

testing$user_name &lt;- factor(testing$user_name)
</code></pre>

<h3>Model Cohort and Cross-Validation Cohort</h3>

<p>The training data set was split up into one portion for model building and another portion for cross-validation.</p>

<ul>
<li>The model cohort will be 70% of the training set.</li>
<li>The cross-validation (CV) cohort will be 30% of the training set.</li>
</ul>

<pre><code class="r">set.seed(12345)

inTrain &lt;- createDataPartition(y = training$classe, p = 0.7, list = FALSE)

model_cohort &lt;- training[inTrain,]

cv_cohort &lt;- training[-inTrain,]
</code></pre>

<h3>Model Building without Utilizing Principal Components Analysis:</h3>

<p>Random forest, boosted tree model and LDA were chosen as the 3 prediction systems used. These models were built on the model cohort and then tested on the cross-validation cohort to determine which model had the best accuracy.</p>

<p>In addition, a stacked model was then developed using the predictions from these three different prediction systems. This
stacked model was developed using the random forest method.</p>

<p>Application of the model on the cross-validation cohort was then used to select the prediction system that had the best 
accuracy on the cross-validation cohort. This then allows us to obtain an estimate of the out of sample error as the
model was not built on any of the cross-validation data. </p>

<p>Models used: </p>

<ul>
<li>Random forest</li>
<li>Boosted tree model</li>
<li>LDA</li>
</ul>

<p>Of note, the random forest model had the best accuracy and was tied with the stacked model. Given that the stacked model 
was more complex, ultimately, the random forest model was chosen as the final model.</p>

<h3>Random Forest Model without Principal Components Processing</h3>

<p>Build the random forest model on the model cohort and then use it to predict the classe on the cross-validation cohort.</p>

<pre><code class="r">rf_model &lt;- train(classe~., method = &quot;rf&quot;, data = model_cohort)

rf_cv_predict &lt;- predict(rf_model, cv_cohort)
</code></pre>

<p>Table of predictions (rows) compared to the true classe (columns)</p>

<pre><code class="r">table(rf_cv_predict, cv_cohort$classe)
</code></pre>

<pre><code>##              
## rf_cv_predict    A    B    C    D    E
##             A 1674    0    0    0    0
##             B    0 1139    1    0    0
##             C    0    0 1025    0    0
##             D    0    0    0  964    2
##             E    0    0    0    0 1080
</code></pre>

<p>The random forest model produces the best accuracy. As will be seen later, it produces the same accuracy as the stacked model.</p>

<pre><code class="r">rf_cv_accuracy &lt;- sum(ifelse(rf_cv_predict == cv_cohort$classe, 1, 0))/length(rf_cv_predict)


rf_cv_accuracy
</code></pre>

<pre><code>## [1] 0.9995
</code></pre>

<h3>Boosted Tree Model without Principal Components Processing</h3>

<p>Build the boosted tree model on the model cohort and then use it to predict the classe on the cross-validation cohort.</p>

<pre><code class="r">gbm_model &lt;- train(classe~., method = &quot;gbm&quot;, data = model_cohort)

gbm_cv_predict &lt;- predict(gbm_model, cv_cohort)
</code></pre>

<p>Table of predictions (rows) compared to the true classe (columns)</p>

<pre><code class="r">table(gbm_cv_predict, cv_cohort$classe)
</code></pre>

<pre><code>##               
## gbm_cv_predict    A    B    C    D    E
##              A 1673    2    0    0    0
##              B    1 1136    1    0    0
##              C    0    1 1016    0    0
##              D    0    0    9  960    3
##              E    0    0    0    4 1079
</code></pre>

<p>The boosted tree model produces the model with the second best accuracy.</p>

<pre><code class="r">gbm_cv_accuracy &lt;- sum(ifelse(gbm_cv_predict == cv_cohort$classe, 1, 0))/length(gbm_cv_predict)

gbm_cv_accuracy
</code></pre>

<pre><code>## [1] 0.9964
</code></pre>

<h3>LDA Model without Principal Components Processing</h3>

<p>Build the LDA model on the model cohort and then use it to predict the classe on the cross-validation cohort.</p>

<pre><code class="r">lda_model &lt;- train(classe~., method = &quot;lda&quot;, data = model_cohort)
</code></pre>

<pre><code>## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
## Warning: variables are collinear
</code></pre>

<pre><code class="r">lda_cv_predict &lt;- predict(lda_model, cv_cohort)
</code></pre>

<p>Table of predictions (rows) compared to the truee classe (columns)</p>

<pre><code class="r">table(lda_cv_predict, cv_cohort$classe)
</code></pre>

<pre><code>##               
## lda_cv_predict    A    B    C    D    E
##              A 1534  153    2    0    0
##              B  126  836  111    0    0
##              C   14  144  877  107    2
##              D    0    6   34  802   94
##              E    0    0    2   55  986
</code></pre>

<p>The LDA model&#39;s accuracy is worse than the boosted tree model.</p>

<pre><code class="r">lda_cv_accuracy &lt;- sum(ifelse(lda_cv_predict == cv_cohort$classe, 1, 0))/length(lda_cv_predict)

lda_cv_accuracy
</code></pre>

<pre><code>## [1] 0.8556
</code></pre>

<h3>Build a stacked random forest model using the above models</h3>

<ul>
<li>This will be the stacked model of the random forest model, boosted tree model and the LDA model</li>
<li>This model will be developed using the random forest method</li>
</ul>

<p>Create a new data frame where the features (predictors) are the predictions of the random forest model, boosted tree model and LDA model. </p>

<pre><code class="r">stacked_model_cohort &lt;- data.frame(model_cohort$classe, predict(rf_model, model_cohort), predict(gbm_model, model_cohort), predict(lda_model, model_cohort))

colnames(stacked_model_cohort)[1] &lt;- &quot;classe&quot;
colnames(stacked_model_cohort)[2] &lt;- &quot;rf_predict&quot;
colnames(stacked_model_cohort)[3] &lt;- &quot;gbm_predict&quot;
colnames(stacked_model_cohort)[4] &lt;- &quot;lda_predict&quot;

stacked_cv_cohort &lt;- data.frame(cv_cohort$classe, predict(rf_model, cv_cohort), predict(gbm_model, cv_cohort), predict(lda_model, cv_cohort))

colnames(stacked_cv_cohort)[1] &lt;- &quot;classe&quot;
colnames(stacked_cv_cohort)[2] &lt;- &quot;rf_predict&quot;
colnames(stacked_cv_cohort)[3] &lt;- &quot;gbm_predict&quot;
colnames(stacked_cv_cohort)[4] &lt;- &quot;lda_predict&quot;
</code></pre>

<p>Build the stacked model on the model cohort and then use it to predict the classe on the cross-validation cohort.</p>

<pre><code class="r">stacked_rf_model &lt;- train(classe~., method = &quot;rf&quot;, data = stacked_model_cohort)

stacked_rf_cv_predict &lt;- predict(stacked_rf_model, stacked_cv_cohort)
</code></pre>

<p>Table of predictions (rows) compared to the true classe (columns)</p>

<pre><code class="r">table(stacked_rf_cv_predict, stacked_cv_cohort$classe)
</code></pre>

<pre><code>##                      
## stacked_rf_cv_predict    A    B    C    D    E
##                     A 1674    0    0    0    0
##                     B    0 1139    1    0    0
##                     C    0    0 1025    0    0
##                     D    0    0    0  964    2
##                     E    0    0    0    0 1080
</code></pre>

<p>The stacked model&#39;s accuracy is the same as the random forest model alone.</p>

<pre><code class="r">stacked_rf_cv_accuracy &lt;- sum(ifelse(stacked_rf_cv_predict == stacked_cv_cohort$classe, 1, 0))/length(stacked_rf_cv_predict)

stacked_rf_cv_accuracy
</code></pre>

<pre><code>## [1] 0.9995
</code></pre>

<h3>Out of Sample Error Estimate of Models without PCA Preprocessing</h3>

<p>Create new data frame of the accuracy and out of sample errors (as performed on the cross-validation cohort) of the different models.</p>

<pre><code class="r">rf_model_oos_error &lt;- data.frame(rf_cv_accuracy, 1 - rf_cv_accuracy)

colnames(rf_model_oos_error)[1] &lt;- &#39;accuracy&#39;
colnames(rf_model_oos_error)[2] &lt;- &#39;out_of_sample_error&#39;

gbm_model_oos_error &lt;- data.frame(gbm_cv_accuracy, 1 - gbm_cv_accuracy)

colnames(gbm_model_oos_error)[1] &lt;- &#39;accuracy&#39;
colnames(gbm_model_oos_error)[2] &lt;- &#39;out_of_sample_error&#39;

lda_model_oos_error &lt;- data.frame(lda_cv_accuracy, 1 - lda_cv_accuracy)

colnames(lda_model_oos_error)[1] &lt;- &#39;accuracy&#39;
colnames(lda_model_oos_error)[2] &lt;- &#39;out_of_sample_error&#39;

stacked_rf_model_oos_error &lt;- data.frame(stacked_rf_cv_accuracy, 1 - stacked_rf_cv_accuracy)

colnames(stacked_rf_model_oos_error)[1] &lt;- &#39;accuracy&#39;
colnames(stacked_rf_model_oos_error)[2] &lt;- &#39;out_of_sample_error&#39;

oos_error_of_models_without_pca &lt;- rbind(rf_model_oos_error, gbm_model_oos_error, lda_model_oos_error, stacked_rf_model_oos_error)

rownames(oos_error_of_models_without_pca)[1] &lt;- &#39;random_forest_model&#39;

rownames(oos_error_of_models_without_pca)[2] &lt;- &#39;boosted_tree_model&#39;

rownames(oos_error_of_models_without_pca)[3] &lt;- &#39;lda_model&#39;

rownames(oos_error_of_models_without_pca)[4] &lt;- &#39;stacked_random_forest_model&#39;
</code></pre>

<p>Data frame that compares the accuracies and out of sample errors on the different models.</p>

<pre><code class="r">oos_error_of_models_without_pca
</code></pre>

<pre><code>##                             accuracy out_of_sample_error
## random_forest_model           0.9995           0.0005098
## boosted_tree_model            0.9964           0.0035684
## lda_model                     0.8556           0.1444350
## stacked_random_forest_model   0.9995           0.0005098
</code></pre>

<h3>Model Building Utilizing Principal Components Analysis:</h3>

<p>To determine if pre-processing by breaking down the predictors by principal components analysis (PCA) provided additional
improvement to the model development, the random forest, boosted tree model, LDA and stacked model were then developed with
preProcess = &ldquo;PCA&rdquo;. </p>

<p>Again, the models were built on the model cohort and tested on the cross-validation cohort. The out of sample error was again estimated by applying the models on the cross-validation cohort. </p>

<p>Unlike above, the stacked model with PCA preprocessing was the most accurate model, beating out the random forest model 
with PCA preprocessing.</p>

<p>However, comparing to the random forest model without PCA preprocessing, the stacked model with PCA preprocessing was
less accurate. As a result, the random forest model without PCA preprocessing was chosen as the final model.</p>

<h3>Random Forest Model with PCA preProcess</h3>

<p>Build the random forest model with PCA preprocessing on the model cohort and then use it to predict the classe on the cross-validation cohort.</p>

<pre><code class="r">rf_pca_model &lt;- train(classe~., method = &quot;rf&quot;, preProcess = &quot;pca&quot;, data = model_cohort)
</code></pre>

<pre><code>## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
## Warning: invalid mtry: reset to within valid range
</code></pre>

<pre><code class="r">rf_pca_cv_predict &lt;- predict(rf_pca_model, cv_cohort)
</code></pre>

<p>Table of predictions (rows) compared to the true classe (columns)</p>

<pre><code class="r">table(rf_pca_cv_predict, cv_cohort$classe)
</code></pre>

<pre><code>##                  
## rf_pca_cv_predict    A    B    C    D    E
##                 A 1660   19    2    0    0
##                 B   13 1108   21    0    0
##                 C    1   12  995   41    0
##                 D    0    0    8  916   11
##                 E    0    0    0    7 1071
</code></pre>

<p>The random forest model with PCA preprocessing has an accuracy that is worse than the original random forest model above.</p>

<pre><code class="r">rf_pca_cv_accuracy &lt;- sum(ifelse(rf_pca_cv_predict == cv_cohort$classe, 1, 0))/length(rf_pca_cv_predict)

rf_pca_cv_accuracy
</code></pre>

<pre><code>## [1] 0.9771
</code></pre>

<h3>Boosted Tree Model with PCA preProcess</h3>

<p>Build the boosted tree model with PCA preprocessing on the model cohort and then use it to predict the classe on the cross-validation cohort.</p>

<pre><code class="r">gbm_pca_model &lt;- train(classe~., method = &quot;gbm&quot;, preProcess = &quot;pca&quot;, data = model_cohort)

gbm_pca_cv_predict &lt;- predict(gbm_pca_model, cv_cohort)
</code></pre>

<p>Table of predictions (rows) compared to the true classe (columns)</p>

<pre><code class="r">table(gbm_pca_cv_predict, cv_cohort$classe)
</code></pre>

<pre><code>##                   
## gbm_pca_cv_predict    A    B    C    D    E
##                  A 1593   92   17    1    0
##                  B   58  967   81    2    0
##                  C   23   80  917   83    3
##                  D    0    0   11  857   62
##                  E    0    0    0   21 1017
</code></pre>

<p>The boosted tree model with PCA preprocessing has an accuracy that is worse than the original boosted tree model.</p>

<pre><code class="r">gbm_pca_cv_accuracy &lt;- sum(ifelse(gbm_pca_cv_predict == cv_cohort$classe, 1, 0))/length(gbm_pca_cv_predict)

gbm_pca_cv_accuracy
</code></pre>

<pre><code>## [1] 0.9093
</code></pre>

<h3>LDA Model with PCA preProcess</h3>

<p>Build the LDA model with PCA preprocessing on the model cohort and then use it to predict the classe on the cross-validation cohort.</p>

<pre><code class="r">lda_pca_model &lt;- train(classe~., method = &quot;lda&quot;, preProcess = &quot;pca&quot;, data = model_cohort)

lda_pca_cv_predict &lt;- predict(lda_pca_model, cv_cohort)
</code></pre>

<p>Table of predictions (rows) compared to the true classe (columns)</p>

<pre><code class="r">table(lda_pca_cv_predict, cv_cohort$classe)
</code></pre>

<pre><code>##                   
## lda_pca_cv_predict    A    B    C    D    E
##                  A 1306  148   13    0    0
##                  B  314  707  233   58    0
##                  C   54  262  704  156    3
##                  D    0   22   55  583  238
##                  E    0    0   21  167  841
</code></pre>

<p>The accuracy of the LDA model with PCA preprocessing is worse than the original LDA model.</p>

<pre><code class="r">lda_pca_cv_accuracy &lt;- sum(ifelse(lda_pca_cv_predict == cv_cohort$classe, 1, 0))/length(lda_pca_cv_predict)

lda_pca_cv_accuracy
</code></pre>

<pre><code>## [1] 0.7037
</code></pre>

<h3>Build a stacked random forest model using the above models (with PCA preprocessing)</h3>

<ul>
<li>This will be the stacked model of the random forest model (with PCA preprocessing), boosted tree model (with PCA preprocessing) and the LDA model (with PCA preprocessing)</li>
<li>This model will be developed using the random forest method</li>
</ul>

<p>Create a new data frame where the features (predictors) are the predictions of the random forest model (with PCA preprocessing), boosted tree model (with PCA preprocessing) and LDA model (with PCA preprocessing). </p>

<pre><code class="r">stacked_model_pca_cohort &lt;- data.frame(model_cohort$classe, predict(rf_pca_model, model_cohort), predict(gbm_pca_model, model_cohort), predict(lda_pca_model, model_cohort))

colnames(stacked_model_pca_cohort)[1] &lt;- &quot;classe&quot;
colnames(stacked_model_pca_cohort)[2] &lt;- &quot;rf_pca_predict&quot;
colnames(stacked_model_pca_cohort)[3] &lt;- &quot;gbm_pca_predict&quot;
colnames(stacked_model_pca_cohort)[4] &lt;- &quot;lda_pca_predict&quot;

stacked_cv_pca_cohort &lt;- data.frame(cv_cohort$classe, predict(rf_pca_model, cv_cohort), predict(gbm_pca_model, cv_cohort), predict(lda_pca_model, cv_cohort))

colnames(stacked_cv_pca_cohort)[1] &lt;- &quot;classe&quot;
colnames(stacked_cv_pca_cohort)[2] &lt;- &quot;rf_pca_predict&quot;
colnames(stacked_cv_pca_cohort)[3] &lt;- &quot;gbm_pca_predict&quot;
colnames(stacked_cv_pca_cohort)[4] &lt;- &quot;lda_pca_predict&quot;
</code></pre>

<p>Build the stacked random forest model using the above models that used PCA preprocessing on the model cohort and then use it to predict the classe on the cross-validation cohort.</p>

<pre><code class="r">stacked_rf_pca_model &lt;- train(classe~., method = &quot;rf&quot;, data = stacked_model_pca_cohort)

stacked_rf_pca_cv_predict &lt;- predict(stacked_rf_pca_model, stacked_cv_pca_cohort)
</code></pre>

<p>Table of predictions (rows) compared to the true classe (columns)</p>

<pre><code class="r">table(stacked_rf_pca_cv_predict, stacked_cv_pca_cohort$classe)
</code></pre>

<pre><code>##                          
## stacked_rf_pca_cv_predict    A    B    C    D    E
##                         A 1660   19    2    0    0
##                         B   13 1108   21    0    0
##                         C    1   12  995   41    0
##                         D    0    0    8  916   11
##                         E    0    0    0    7 1071
</code></pre>

<p>The stacked model using the previous models with PCA preprocessing has an accuracy that is equal to the random forest model with PCA processing.</p>

<pre><code class="r">stacked_rf_pca_cv_accuracy &lt;- sum(ifelse(stacked_rf_pca_cv_predict == stacked_cv_pca_cohort$classe, 1, 0))/length(stacked_rf_pca_cv_predict)

stacked_rf_pca_cv_accuracy
</code></pre>

<pre><code>## [1] 0.9771
</code></pre>

<h3>Out of Sample Error Estimate of Models with PCA Preprocessing</h3>

<p>Create new data frame of the accuracy and out of sample errors (as performed on the cross-validation cohort) of the different models.</p>

<pre><code class="r">rf_pca_model_oos_error &lt;- data.frame(rf_pca_cv_accuracy, 1 - rf_pca_cv_accuracy)

colnames(rf_pca_model_oos_error)[1] &lt;- &#39;accuracy&#39;
colnames(rf_pca_model_oos_error)[2] &lt;- &#39;out_of_sample_error&#39;

gbm_pca_model_oos_error &lt;- data.frame(gbm_pca_cv_accuracy, 1 - gbm_pca_cv_accuracy)

colnames(gbm_pca_model_oos_error)[1] &lt;- &#39;accuracy&#39;
colnames(gbm_pca_model_oos_error)[2] &lt;- &#39;out_of_sample_error&#39;

lda_pca_model_oos_error &lt;- data.frame(lda_pca_cv_accuracy, 1 - lda_pca_cv_accuracy)

colnames(lda_pca_model_oos_error)[1] &lt;- &#39;accuracy&#39;
colnames(lda_pca_model_oos_error)[2] &lt;- &#39;out_of_sample_error&#39;

stacked_rf_pca_model_oos_error &lt;- data.frame(stacked_rf_pca_cv_accuracy, 1 - stacked_rf_pca_cv_accuracy)

colnames(stacked_rf_pca_model_oos_error)[1] &lt;- &#39;accuracy&#39;
colnames(stacked_rf_pca_model_oos_error)[2] &lt;- &#39;out_of_sample_error&#39;

oos_error_of_models_with_pca &lt;- rbind(rf_pca_model_oos_error, gbm_pca_model_oos_error, lda_pca_model_oos_error, stacked_rf_pca_model_oos_error)

rownames(oos_error_of_models_with_pca)[1] &lt;- &#39;random_forest_model_with_pca&#39;

rownames(oos_error_of_models_with_pca)[2] &lt;- &#39;boosted_tree_model_with_pca&#39;

rownames(oos_error_of_models_with_pca)[3] &lt;- &#39;lda_model_with_pca&#39;

rownames(oos_error_of_models_with_pca)[4] &lt;- &#39;stacked_random_forest_model_with_pca&#39;
</code></pre>

<p>Data frame that compares the accuracies and out of sample errors on the different models. Of note, these models were performed using PCA preprocessing.</p>

<pre><code class="r">oos_error_of_models_with_pca
</code></pre>

<pre><code>##                                      accuracy out_of_sample_error
## random_forest_model_with_pca           0.9771             0.02294
## boosted_tree_model_with_pca            0.9093             0.09074
## lda_model_with_pca                     0.7037             0.29635
## stacked_random_forest_model_with_pca   0.9771             0.02294
</code></pre>

<p>For comparison, here is the data frame of results from the model comparisons without PCA preprocessing.</p>

<pre><code class="r">oos_error_of_models_without_pca
</code></pre>

<pre><code>##                             accuracy out_of_sample_error
## random_forest_model           0.9995           0.0005098
## boosted_tree_model            0.9964           0.0035684
## lda_model                     0.8556           0.1444350
## stacked_random_forest_model   0.9995           0.0005098
</code></pre>

<p>The best model performance on the cross-validation cohort with the smallest out of sample error is the random forest model and the stacked random forest model.</p>

<p>As a result, PCA preprocessing is not needed and the random forest model will be selected as the final model because, of the models with the best accuracy, it was the simpler model.</p>

<h3>Final Model is the Random Forest Model without PCA</h3>

<p>Given that the random forest model (without PCA) produces the same result as the stacked model, the random forest model was chosen as the final model for simplicity</p>

<h3>Out of Sample Error</h3>

<p>The random forest model was chosen as the final model given that it was the most accurate model with an accuracy of 0.9994902.</p>

<p>Therefore, the out of sample error (as applied to the cross-validation cohort) is 0.05098% for the random forest model.</p>

<h3>Testing of the best model on the testing cohort:</h3>

<p>Given that the random forest model had the highest accuracy on the cross-validation cohort, it was chosen as the final model.</p>

<p>At this point, the random forest model was tested for the first and only time on the test cohort that had previously
been set aside for testing.</p>

<p>Recall that the random forest model was the most accurate model on the cross-validation cohort, hence it was chosen as the final model.</p>

<ul>
<li>test date frame used: testing</li>
</ul>

<pre><code class="r"># Column 59 of testing is the problem_id

rf_test_predict &lt;- predict(rf_model, testing[, -59])
</code></pre>

<p>The results of the random forest model (final model chosen) on the test data demonstrates that it is able to correctly predict the classe of all 20 test cases.</p>

<pre><code class="r">test_result &lt;- data.frame(testing$problem_id, rf_test_predict)

colnames(test_result)[1] &lt;- &#39;problem_id&#39;

test_result
</code></pre>

<pre><code>##    problem_id rf_test_predict
## 1           1               B
## 2           2               A
## 3           3               B
## 4           4               A
## 5           5               A
## 6           6               E
## 7           7               D
## 8           8               B
## 9           9               A
## 10         10               A
## 11         11               B
## 12         12               C
## 13         13               B
## 14         14               A
## 15         15               E
## 16         16               E
## 17         17               A
## 18         18               B
## 19         19               B
## 20         20               B
</code></pre>

</body>

</html>
